---
layout: post
title:  "Optimize inference times for transformer architectures"
date:   2020-02-05 18:30:15 -0800
categories: jekyll update
---

While the claim to fame of various transformer-like architectures is that. 

# References:
1. [PoWER-BERT paper](https://arxiv.org/pdf/2001.08950.pdf)
2. [Structured Pruning paper](https://arxiv.org/pdf/1910.04732v1.pdf)
3. [Q8-BERT](https://arxiv.org/pdf/1910.06188.pdf)
4. [Reweighted Proximal Pruning](https://openreview.net/pdf?id=r1gBOxSFwr)
5. [LM size reduction using Pruning](https://www.aclweb.org/anthology/P02-1023.pdf)
6. [Pruning a language model in PyTorch](https://nervanasystems.github.io/distiller/tutorial-lang_model.html)
