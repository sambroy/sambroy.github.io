---
layout: post
title:  "Papers of personal interest in ACL 2020"
date:   2020-07-28 18:30:15 -0800
categories: jekyll update
---

Here is a list of some papers of personal interest in ACL 2020. I will write up some
short notes on each of these papers as time goes by.

### Paper list
Here is [the list](https://acl2020.org/program/accepted/) of accepted papers.

* [BLEURT paper](https://www.aclweb.org/anthology/2020.acl-main.704/) - new automatic metrics for NLG evaluation.
  * [google blog here](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)
  * [code for the metric here](https://github.com/google-research/bleurt)
  * [my notes here](bleurt.html)
* [mixture of h-1 heads better than h](https://www.aclweb.org/anthology/2020.acl-main.587/) - transformer  architectures.
  * [my notes here](mixture-of-heads.html)
* [generative model for joint NLU NLG](https://www.aclweb.org/anthology/2020.acl-main.163/)
* [hierarchy of RNN architectures](https://www.aclweb.org/anthology/2020.acl-main.43.pdf)
* [machine reading comprehension framework for NER](https://www.aclweb.org/anthology/2020.acl-main.519.pdf) - handles flat and nested NER.
  * [code here](https://github.com/ShannonAI/mrc-for-flat-nested-ner)
* [BART - denoising seq2seq pretraining](https://www.aclweb.org/anthology/2020.acl-main.703/)
* [bridging structural gap between encoding and decoding in Data-to-text generation](https://www.aclweb.org/anthology/2020.acl-main.224/)
* [distilling bert knowledge for generation](https://www.aclweb.org/anthology/2020.acl-main.705.pdf)
  * [openreview](https://openreview.net/forum?id=Bkgz_krKPB)
  * [code](https://github.com/castorini/d-bert)
* [role of attention heads in transformer architectures](https://www.aclweb.org/anthology/2020.acl-main.311/#:~:text=Abstract,is%20unique%20in%20the%20architecture.)
* [injecting numeracy into language models](https://www.aclweb.org/anthology/2020.acl-main.89.pdf)
  * [code](https://github.com/ag1988/injecting_numeracy)

### References
* [Papers with code for data-to-text generation](https://paperswithcode.com/task/data-to-text-generation/codeless)
* [NLP reading list](https://github.com/changwookjun/nlp-paper#probe)
