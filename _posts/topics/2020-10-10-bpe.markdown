---
layout: post
title:  "bpe related papers"
date:   2020-07-28 18:30:15 -0800
categories: jekyll update
---

notes on bpe.

1. bpe is deterministic. given corpus, find frequent byte pairs. bpe breaks segment into
unique sequences. We stop the bpe process according to the vocab size we want.

The good:
* simple, easy to understand segmentation, easy to implement

The not so good:
* we would ideally like to give an option to the training algorithm - since we are working
at the level of subwords, that is already a bit nebulous. Are we absolutely confident about
the subword segmentation that our algorithm has chosen, and use that to train our LM?
**Bottomline**: we would like options for the segmentation.

How is this mitigated in practice?
* after the bpe paper came out in nmt, the subword regularization paper came out.
This proposes a way (sentencepiece) that makes it possible to get multiple segmentations
for a given word sequence.
However, this is a bit complicated.
* the bpe-dropout paper makes a simple tweak on the bpe idea, introducing a randomness
step (a _dropout_ step) and this again makes it possible to have multiple segmentations
for a given word sequence.

### sequence to read
1. http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html
2. https://jlibovicky.github.io/2019/11/07/MT-Weekly-BPE-dropout.html
3. https://www.groundai.com/project/bpe-dropout-simple-and-effective-subword-regularization/1#S4.T1

### Paper list
* [bpe in nmt](https://arxiv.org/pdf/1508.07909.pdf)
  * [code](https://github.com/rsennrich/subword-nmt)
* [notes on bpe](http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html)
* [bpe dropout](https://arxiv.org/pdf/1910.13267.pdf)
  * [blog](https://jlibovicky.github.io/2019/11/07/MT-Weekly-BPE-dropout.html)
  * [also see this](https://www.groundai.com/project/bpe-dropout-simple-and-effective-subword-regularization/1)
  * [code](https://github.com/rsennrich/subword-nmt#advanced-features)
