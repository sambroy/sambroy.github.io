---
layout: post
title:  "bpe related papers"
date:   2020-10-10 18:30:15 -0800
categories: jekyll update
---


## overview of basic bpe
the [wikipedia page](https://en.wikipedia.org/wiki/Byte_pair_encoding) gives a good
overview of what bpe (byte pair encoding) is about. in short, it is a form of
data compression where we compress string data using the _most frequent_
substrings/subwords.

the idea is well motivated:
1. the main attempt is to take into account the **compositionality** of words. As one
example of such compositionality, consider _declension_: when the form of a noun, pronoun, adjective, or article changes to indicate number, grammatical case, or gender (see [wiki](https://en.wikipedia.org/wiki/Declension) or [here](https://study.com/academy/lesson/declension-definition-examples.html#:~:text=For%20example%2C%20in%20a%20sentence,The%20ball%20was%20his.)).
For instance in English, we [pluralize words](https://www.grammarly.com/blog/plural-nouns/#:~:text=Plural%20Noun%20Rules) by adding an `s` (eg. dogs, cats)
or by adding `es` (eg. buses, taxes, lunches), among other variants (eg. life -> lives).

2. such subword encodings are commonly used in language models (eg. GPT-3); given this is an
attempt to account for the natural compositionality of language, subwords help improve
the _oov_ (out of vocabulary) problem.
with a relatively small vocabulary (consisting
of such subwords), the model will be able to compose various words from subword
tokens, where such words might otherwise have been accounted for as oov.


First, some questions & comments:

1. Here is the example from [wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding) .
We start with
```python
aaabdaaabac
```
and we find that the byte pair `aa` occurs most often, and we replace it with a
byte not used in the data, say `Z`. We thereby get `ZabdZabac`.
  * Note, that we have made some choices here: we adopted a greedy-first way of marking the byte pair `aa`. We could also have gone as `aZbdaZbac`. The wiki page also proceeds with _recursive_ byte pair encoding (which is what is used in common practice). Also find here, [Rico Sennrich's example of a
  set of bpe codes](https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/tests/data/bpe.ref).
Can this make a difference in the overall encoding?

2. Note that in the example, we chose the byte pairs according to the string to
be encoded. Usually, we run a stage where we derive the "bpe codes" by taking a
pass through the entire data to find the most frequent byte pairs.

3. Note that we usually denote such bpe codes as _pairs_: so the bpe pair `o ther</w>`
indicates that the two parts of the pair will be _merged_ to produce the encoding.
Why do we indicate the pair, when all we seem to need is the outcome of the merge?
This is for book-keeping. This helps us track the sequence of merges executed
in obtaining a word.

### bpe dropout paper

1. bpe is deterministic. given corpus, find frequent byte pairs. bpe breaks segment into
unique sequences. We stop the bpe process according to the vocab size we want.

#### the good:
* simple, easy to understand segmentation, [easy to implement](http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html)

#### the not so good:
* we would ideally like to give an option to the training algorithm - since we are working
at the level of subwords, that is already a bit nebulous. Are we absolutely confident about
the subword segmentation that our algorithm has chosen, and use that to train our LM?
**Bottomline**: we would like options for the segmentation.

#### How is this mitigated in practice?
* after the bpe paper came out in nmt, the subword regularization paper came out.
This proposes a way (sentencepiece) that makes it possible to get multiple segmentations
for a given word sequence.
However, this is a bit complicated.
* the bpe-dropout paper makes a simple tweak on the bpe idea, introducing a randomness
step (a _dropout_ step) and this again makes it possible to have multiple segmentations
for a given word sequence.
  * in the bpe algorithm, you _merge_ byte pairs in the text. Now, for dropout, you randomly
  pick some of these _merges_ to actually execute.
  * this is what gives multiple alternatives for segmentations for a word sequence.

#### Interesting side-effects:
* bpe-dropout is robust towards mis-spellings. This point is brought out in this [blog](https://jlibovicky.github.io/2019/11/07/MT-Weekly-BPE-dropout.html).
* Normally, we use a decoder to search through the possibilities for the output text.
Typically we use a greedy decoder or a beam search tree. However, bpe regularization provides another viewpoint:
  * we can utilize the different segmentations of the context according to bpe-dropout (or sentencepiece)
  and use the different segmentations to generate suggestions (in an auto-completion scenario).
  * This gives us options instead of relying on beam search to provide multiple options.

### sequence to read
1. [check out the wiki page :)](https://en.wikipedia.org/wiki/Byte_pair_encoding)
2. [bpe implementation, in a notebook by Ethen Liu](http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html)
3. [blog on bpe dropout](https://jlibovicky.github.io/2019/11/07/MT-Weekly-BPE-dropout.html)
4. [the bpe dropout paper](https://www.groundai.com/project/bpe-dropout-simple-and-effective-subword-regularization/1#S4.T1)
5. [implementation of the bpe dropout, by Rico Sennrich](https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/apply_bpe.py)
6. [the original gage 1994 paper on bpe](https://www.derczynski.com/papers/archive/BPE_Gage.pdf)
